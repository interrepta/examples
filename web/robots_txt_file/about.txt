"Plik robots.txt pozwala kontrolować dostęp robotów do plików w Twojej witrynie. Plik robots.txt znajduje się w katalogu głównym witryny. W przypadku witryny www.example.com plik robots.txt znajduje się pod adresem www.example.com/robots.txt. Ma on format zwykłego tekstu i jest zgodny ze standardem Robots Exclusion Protocol. Plik robots.txt zawiera co najmniej 1 regułę. Każda reguła blokuje lub umożliwia dostęp określonego robota do wskazanego pliku w domenie lub subdomenie, w której jest przechowywany plik robots.txt. Domyślnie wszystkie pliki mogą być skanowane, o ile nie określisz inaczej w pliku robots.txt. "

"

Zasady dotyczące formatu i lokalizacji:

- Plik musi się nazywać robots.txt.
- Witryna może mieć tylko 1 plik robots.txt.
- Plik robots.txt musi się znajdować w katalogu głównym hosta witryny, do której się odnosi. Aby na przykład kontrolować indeksowanie wszystkich adresów URL witryny https://www.example.com/, plik robots.txt musi mieć lokalizację https://www.example.com/robots.txt. Nie można go umieścić w podkatalogu (na przykład https://example.com/pages/robots.txt). Jeśli nie wiesz, jak uzyskać dostęp do katalogu głównego witryny, lub potrzebujesz do tego uprawnień, skontaktuj się ze swoim dostawcą hostingu WWW. Jeśli nie masz dostępu do katalogu głównego witryny, możesz zablokować roboty w inny sposób, np. przy użyciu metatagów.
- Plik robots.txt może być publikowany w subdomenie (np. https://website.example.com/robots.txt) lub w portach niestandardowych (np. http://example.com:8181/robots.txt).
- Plik robots.txt dotyczy tylko ścieżek w ramach protokołu, hosta i portu, gdzie został opublikowany. Oznacza to, że reguły w pliku https://example.com/robots.txt będą mieć zastosowanie tylko do plików na https://example.com/, a nie do subdomen (takich jak https://m.example.com/) czy alternatywnych protokołów, takich jak http://example.com/.
- Plik robots.txt musi być plikiem tekstowym zakodowanym w formacie UTF-8 (który zawiera znaki ASCII). Google może ignorować znaki spoza zakresu UTF-8, przez co reguły w pliku robots.txt mogą być renderowane nieprawidłowo.


"

"

Dodawanie reguł do pliku robots.txt

Reguły to instrukcje wskazujące robotom, które części witryny mogą indeksować. Podczas dodawania reguł do pliku robots.txt postępuj zgodnie z tymi wskazówkami:

- Plik robots.txt zawiera co najmniej 1 grupę.
- Każda grupa składa się z wielu reguł lub dyrektyw (instrukcji), przy czym w 1 wierszu może znajdować się 1 dyrektywa. Każda grupa zaczyna się od wiersza User-agent, który określa cel grupy.
- Grupa zawiera te informacje:
-- do kogo ma zastosowanie grupa (do jakiego klienta użytkownika),
-- do których katalogów lub plików klient użytkownika może uzyskać dostęp,
-- do których katalogów lub plików klient użytkownika nie może uzyskać dostępu.
- Roboty przetwarzają grupy od góry do dołu. Klient użytkownika jest dopasowywany tylko do 1 zestawu reguł – do pierwszej najmocniej sprecyzowanej grupy, która się do niego odnosi.
- Z założenia klient użytkownika może indeksować wszystkie strony lub katalogi, które nie są zablokowane przy użyciu reguły disallow.
- Wielkość liter w regułach jest rozróżniana. Na przykład reguła disallow: /file.asp ma zastosowanie do witryny https://www.example.com/file.asp, ale do witryny https://www.example.com/FILE.asp już nie.
    Znak # oznacza początek komentarza.


"

Źródło tekstu: https://developers.google.com/search/docs/crawling-indexing/robots/create-robots-txt
